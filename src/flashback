#!/usr/bin/python -B

import os
import datetime
import subprocess
import shlex
import glob
import operator
import sys
from optparse import OptionParser
import time
import shutil
import imp
import errno
import signal

#-----------------------------------------------------------
#  H E L P E R   C L A S S E S
#-----------------------------------------------------------

def enum(**enums):
    return type('Enum', (), enums)

#-----------------------------------------------------------
#  S T A R T
#-----------------------------------------------------------

# globals
programName = 'flashback'
libdir = '/var/lib/'+programName
maxAge = 10000000
sleepMin = 10
# globalCfg (global)
globalCfg = {}
globalCfg['topDir'] = '/flashback'
globalCfg['logFile'] = '/dev/stdout'
globalCfg['lastBackupTimestamp'] = datetime.datetime(1970,1,1)
globalCfg['cycleSec'] = 24*60*60
globalCfg['keepCount'] = 9
globalCfg['label'] = 'daily'
globalCfg['tool'] = 'rsync'
globalCfg['rsyncOpts'] = ''
globalCfg['removable'] = False
# plumbing (globals)
options = ()
g_logFD = None

jobStatus = enum(
    UNKNOWN    = 'UNKNOWN',
    NOT_READY  = 'NOT_READY',
    READY      = 'READY',
    NOT_FOUND  = 'NOT_FOUND',
    BACKING_UP = 'BACKING_UP',
    ROTATING   = 'ROTATING',
    SUCCEEDED  = 'SUCCEEDED',
    FAILED     = 'FAILED',
    DISABLED   = 'DISABLED'
)

#-----------------------------------------------------------

def jobfile_to_array(filename):
    try:
        log_debug("job file '"+filename+"' found")
        configFilePtr = open( filename, "r" )
    except OSError:
        log_debug("job file '"+filename+"' not found")
        sys.exit(1)

    # MAKE BASIC ENTRIES IN THE 'jobInfo' DICTIONARY FROM /etc/flashback.jobs

    sanitizedJobs = []
    for line in configFilePtr:
        line = line.partition('#')[0]  # strip comments
        line = line.rstrip('\r\n')     # strip trailing CR/LF
        line = line.strip(' ')         # strip leading space
        log_debug(filename+": "+line)
        pieces = line.split(' ')
        rawJob = {}   # raw junk read from config file
        for piece in pieces:
            keyValuePair = piece.split('=')
            if len(keyValuePair) == 2:
                key = keyValuePair[0]
                value = keyValuePair[1]
                rawJob[key] = value
        if len(rawJob) == 0:
            continue
        log_debug('raw job: '+','.join(['%s>%s' % (key, value) for (key, value) in rawJob.items()]) )

        sanitizedJob = {}   # sanitized job description
        # STRING OPTIONS
        req1 = setParmString(filename,rawJob,'host',sanitizedJob,'host')
        req2 = setParmString(filename,rawJob,'volume',sanitizedJob,'volume')
        req3 = setParmString(filename,rawJob,'src',sanitizedJob,'src')
        setParmString(filename,rawJob,'fqdn',sanitizedJob,'fqdn')
        setParmString(filename,rawJob,'label',sanitizedJob,'label')
        setParmString(filename,rawJob,'rsyncOpts',sanitizedJob,'rsyncOpts')
        setParmString(filename,rawJob,'tool',sanitizedJob,'tool')
        # NUMBER OPTIONS
        setParmInt(filename,rawJob,'cycleDay',sanitizedJob,'cycleSec',86400)
        setParmInt(filename,rawJob,'cycleHour',sanitizedJob,'cycleSec',3600)
        setParmInt(filename,rawJob,'cycleMin',sanitizedJob,'cycleSec',60)
        setParmInt(filename,rawJob,'cycleSec',sanitizedJob,'cycleSec')
        setParmInt(filename,rawJob,'keepCount',sanitizedJob,'keepCount')
        # BOOLEAN OPTIONS
        setParmBool(filename,rawJob,'disabled',sanitizedJob,'disabled')
        setParmBool(filename,rawJob,'removable',sanitizedJob,'removable')

        # Specifying an FQDN is optional in the config file.
        # If none was specified, then set it to the host.
        if 'fqdn' not in sanitizedJob:
            sanitizedJob['fqdn'] = sanitizedJob['host']
        # If host is in FQDN format (with dots), trim them out.
        if sanitizedJob['host'].find('.') >= 0:
            sanitizedJob['host'] = sanitizedJob['host'].split('.')[0]

        # IF THE JOB INFO IS COMPLETE, THEN SAVE IT
        if len(sanitizedJob) > 0:
            if req1 and req2 and req3:
                log_debug('sanitizedJob:'+','.join(['%s>%s' % (key, value) for (key, value) in sanitizedJob.items()]) )
                sanitizedJobs.append(sanitizedJob)
            else:
                log_error('incomplete job '+','.join(['%s>%s' % (key, value) for (key, value) in sanitizedJob.items()]) )

        # IF THE JOB CONTAINS ERRORS, THEN STOP
        if len(rawJob) > 0:
            for key in rawJob.keys():
                log_error("unknown job parameter '"+key+"' in '"+filename+"'")
            sys.exit(1)

    log_debug('end of '+filename)
    return sanitizedJobs

#-----------------------------------------------------------

def cfgfile_to_dict(filename):
    try:
        log_debug("config file '"+filename+"' found")
        handle = open( filename, "r" )
    except OSError:
        log_debug("config file '"+filename+"' not found")
        return dict()

    # PARSE CONFIG FILE, BUILD DICTIONARY

    raw = {}
    for line in handle:
        line = line.partition('#')[0]  # strip comments
        line = line.rstrip('\r\n')     # strip trailing CR/LF
        line = line.strip(' ')         # strip leading space
        if len(line) == 0:
            continue
        if line.find('=') == -1:
            log_error('syntax error in config file '+filename+' > '+line)
            sys.exit(1)
        log_debug(filename+": "+line)
        (key,eq,value) = line.partition('=')
        key = key.strip()
        value = value.strip()
        raw[key] = value
        log_debug(filename+" "+key+":"+value)

    handle.close()
    log_debug('raw '+','.join(['%s:%s' % (key, value) for (key, value) in raw.items()]) )
    return raw

#-----------------------------------------------------------

def log_init(logfile=None):
    # we're writing these globals
    global g_logFD
    # log file
    if g_logFD:
        g_logFD.close()
    if logfile is not None:
        g_logFD = open(logfile,'a')
    else:
        g_logFD = open('/dev/stderr','a')

#-----------------------------------------------------------

def log_debug(string):
    if options.debug:
        log_info(string)

#-----------------------------------------------------------

def log_info(string):
    if options.quiet:
        return
    global g_logFD
    timeStamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    g_logFD.write(timeStamp+" "+string+"\n")
    g_logFD.flush()
    ##os.fsync(g_logFD)

#-----------------------------------------------------------

def log_error(string):
    global g_logFD
    timeStamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    g_logFD.write(timeStamp+" ERROR "+string+"\n")
    g_logFD.flush()
    ##os.fsync(g_logFD)

#-----------------------------------------------------------

def format_arglist_into_shellcmd(originalArgs):
    copyOfArgs = list(originalArgs)
    for idx, arg in enumerate(copyOfArgs):
        if arg.find(' ') != -1:
            modifiedArg = '"'+arg.replace('"','\\"')+'"'
            copyOfArgs[idx] = modifiedArg
    return (' '.join(copyOfArgs))

#-----------------------------------------------------------

def shell_capture(cmdargs):
    global g_logFD
    log_debug('shell_capture arguments >> '+(",".join(cmdargs)))
    log_debug('shell_capture command >> '+format_arglist_into_shellcmd(cmdargs))
    p = subprocess.Popen(cmdargs, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = p.communicate()
    rc = p.returncode
    log_debug("shell_capture done, rc="+('%d'%rc))
    return rc, stdout, stderr

#-----------------------------------------------------------

def shell_do(cmdargs):
    global g_logFD
    log_debug('shell_do arguments >> '+(",".join(cmdargs)))
    log_debug('shell_do command >> '+format_arglist_into_shellcmd(cmdargs))
    rc = subprocess.call(cmdargs)
    log_debug("shell_do rc = "+("%d"%rc))
    return rc

#-----------------------------------------------------------

def sec2dhms(s):
    days = s // 86400  ; s = s - (days * 86400)
    hours = s // 3600  ; s = s - (hours * 3600)
    mins = s // 60     ; s = s - (mins * 60)
    secs = s
    return (days, hours, mins, secs)

#-----------------------------------------------------------

def sec2string(s):
    (d,h,m,s) = sec2dhms(s)
    if d > 0:
        return '%dd+%d:%02d:%02d' % (d,h,m,s)
    else:
        return '%d:%02d:%02d' % (h,m,s)

#-----------------------------------------------------------

def ping(host):
    return True if ( shell_do(['ping','-c1',host]) == 0 ) else False

#-----------------------------------------------------------

def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc : # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

#-----------------------------------------------------------

def disk_usage(path):
    """Return disk usage statistics about the given path.

    Returned value is a tuple with three attributes:
    'total', 'used' and 'free' (in bytes).
    """
    st = os.statvfs(path)
    free = st.f_bavail * st.f_frsize
    total = st.f_blocks * st.f_frsize
    used = (st.f_blocks - st.f_bfree) * st.f_frsize
    return (total, used, free)

#-----------------------------------------------------------

def setParmString(cfgFile,cfgHash,cfgIdx,volHash,volIdx):
    if cfgIdx not in cfgHash : return False
    volHash[volIdx] = cfgHash[cfgIdx]
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> STRING >> '+volIdx+'='+volHash[volIdx])
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def setParmInt(cfgFile,cfgHash,cfgIdx,volHash,volIdx,multiplier=1):
    if cfgIdx not in cfgHash : return False
    volHash[volIdx] = int(cfgHash[cfgIdx]) * multiplier
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> INT >> '+volIdx+'='+str(volHash[volIdx]))
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def setParmBool(cfgFile,cfgHash,cfgIdx,volHash,volIdx):
    if cfgIdx not in cfgHash : return False
    if cfgHash[cfgIdx].lower() in ['yes','y','true','1']:
        volHash[volIdx] = True
    else:
        volHash[volIdx] = False
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> BOOL >> '+volIdx+'='+( 'TRUE' if volHash[volIdx] else 'FALSE'))
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def buildJobTable():

    jobInfo = jobfile_to_array('/etc/flashback.jobs')

    # FILL IN THE 'jobInfo' DICTIONARY FROM globalCfg AND JOB-SPECIFIC config FILES

    for job in jobInfo:

        log_debug('job:'+','.join(['%s>%s' % (key, value) for (key, value) in job.items()]) )

        # set some defaults - always
        job['status'] = jobStatus.UNKNOWN
        job['lastBackupDurationSec'] = 0
        # set some defaults - only if not set in main jobs config file (TODO: change this ???)
        for i in ['label','cycleSec','keepCount','tool','rsyncOpts','removable']:
            if i not in job : job[i] = globalCfg[i]
        if 'disabled' not in job : job['disabled'] = False

        # set an index to refer to this entry by
        index = job['host']+'-'+job['volume']+'-'+job['label']
        job['index'] = index

        # build a list of several machine-specific and job-specific config files to look for
        configFilenames=[]
        configFilenames.append(globalCfg['topDir']+'/config') # /backup/config
        configFilenames.append(globalCfg['topDir']+'/'+job['host']+'/config') # /backup/host/config
        configFilenames.append(globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/config') # /backup/host/volume/config

        for configFilename in configFilenames:
            # read the config files if they exist
            if os.path.isfile(configFilename):
                log_debug("reading config file '"+configFilename+"'")
                rawConfig = cfgfile_to_dict(configFilename)
                # STRING OPTIONS
                setParmString(configFilename,rawConfig,'src',job,'src')
                setParmString(configFilename,rawConfig,'label',job,'label')
                setParmString(configFilename,rawConfig,'rsyncOpts',job,'rsyncOpts')
                setParmString(configFilename,rawConfig,'tool',job,'tool')
                # NUMBER OPTIONS
                setParmInt(configFilename,rawConfig,'cycleDay',job,'cycleSec',86400)
                setParmInt(configFilename,rawConfig,'cycleHour',job,'cycleSec',3600)
                setParmInt(configFilename,rawConfig,'cycleMin',job,'cycleSec',60)
                setParmInt(configFilename,rawConfig,'cycleSec',job,'cycleSec')
                setParmInt(configFilename,rawConfig,'keepCount',job,'keepCount')
                # BOOLEAN OPTIONS
                setParmBool(configFilename,rawConfig,'disabled',job,'disabled')

                if len(rawConfig) > 0:
                    for key in rawConfig.keys():
                        log_error("unknown parameter '"+key+"' in '"+configFilename+"'")
                    sys.exit(1)

            else:
                log_debug("no config file '"+configFilename+"'")

        # Get the creation time of the daily.1 directory, if it exists.
        recentBackup = globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.1'
        # Note: ctime() does not refer to creation time on *nix systems,
        # but rather the last time the inode data changed.
        if os.path.exists(recentBackup):
            mtime = os.path.getmtime(recentBackup)
            job['lastBackupTimestamp'] = datetime.datetime.fromtimestamp(mtime)
        else:
            job['lastBackupTimestamp'] = globalCfg['lastBackupTimestamp']

    return jobInfo

#-----------------------------------------------------------

def updateAgesAndSort(jobInfo):

    # GO THROUGH THE LIST IN ORDER, DETERMINE THEIR AGES AND NEXT BACKUP TIME

    now = datetime.datetime.now()
    for job in jobInfo:
        ageDelta = now - job['lastBackupTimestamp']
        job['ageSec'] = ageDelta.seconds + (ageDelta.days * 86400)
        job['nextBackupSec'] = job['cycleSec'] - job['ageSec']
        log_debug('index='+job['index']+', nextBackupSec='+str(job['nextBackupSec'])+'sec')
        # force disabled backups to the bottom of the list
        if job['disabled'] : job['nextBackupSec'] = maxAge

    for job in jobInfo:
        if job['status'] in (jobStatus.NOT_READY, jobStatus.UNKNOWN):
            # If "cycleSec" has transpired since our last backup, we're "ready".
            if job['ageSec'] > job['cycleSec'] : job['status'] = jobStatus.READY
            else : job['status'] = jobStatus.NOT_READY
        # No matter if "ready" or not, if disabled, don't back up.
        if job['disabled']: job['status'] = jobStatus.DISABLED

    sortedJobs = sorted(jobInfo, key=operator.itemgetter('nextBackupSec'))

    return sortedJobs

#-----------------------------------------------------------

def formattedTable(jobs):
    widths = { 'index':0, 'lastBackupTimestamp':0, 'ageSec':0, 'cycleSec':0 }
    for job in jobs:
        for fld in ('index','ageSec','cycleSec'):
            widths[fld] = max(widths[fld], len(str(job[fld])))
    widths['lastBackupTimestamp'] = len('2013-03-20 21:05:18')

    table = []
    table.append(''
        + 'INDEX'.center(widths['index'])                     + '   '
        + 'LAST BACKUP'.center(widths['lastBackupTimestamp']) + '   '
        + 'AGE'.center(widths['ageSec'])                      + '/'
        + 'CYCLE'.center(widths['cycleSec'])                  + '   '
        + 'STATUS')

    now = datetime.datetime.now()
    for job in jobs:
        nextBackupInterval = sec2string( job['nextBackupSec'] )
        lastBackupDurationStr = sec2string( job['lastBackupDurationSec'] )
        switch = {
            jobStatus.UNKNOWN :    '???',
            jobStatus.DISABLED :   'DISABLED',
            jobStatus.NOT_READY :  'NEXT RUN IN '+nextBackupInterval,
            jobStatus.NOT_FOUND :  'NOT FOUND',
            jobStatus.BACKING_UP : 'BACKING UP',
            jobStatus.ROTATING :   'ROTATING',
            jobStatus.READY :      'READY',
            jobStatus.SUCCEEDED :  'SUCCEEDED ('+lastBackupDurationStr+')',
            jobStatus.FAILED :     'FAILED ('+lastBackupDurationStr+')',
        }
        lastBackupTimestampString = job['lastBackupTimestamp'].strftime('%Y-%m-%d %H:%M:%S')
        table.append(''
            + job['index'].ljust(widths['index'])            + '   '
            + lastBackupTimestampString                      + '   '
            + str(job['ageSec']).rjust(widths['ageSec'])     + '/'
            + str(job['cycleSec']).ljust(widths['cycleSec']) + '   '
            + switch[job['status']]
        )
    return table

#-----------------------------------------------------------

def reportStatusAndQueue(health,activity,target,waitTime,jobInfo):

    log_debug('writing status file and queue file')
    suffix = str(os.getpid())

    # CREATE A SMALL STATUS FILE

    statusFileName = libdir+'/status'
    statusFile = open(statusFileName+suffix,'w')
    now = datetime.datetime.now()
    # show time, date and PID - so we know that the info is current
    statusFile.write('date='+datetime.datetime.strftime(now,'%Y-%m-%d')+'\n')
    statusFile.write('time='+datetime.datetime.strftime(now,'%H:%M:%S')+'\n')
    statusFile.write('pid='+str(os.getpid())+'\n')
    # show what we're doing (a verb)
    statusFile.write('status='+health+'\n')
    # show what we're doing it to (an object)
    if target is None : target=''
    statusFile.write('target='+target+'\n')
    # show how long we'll do it (only if we're sleeping)
    statusFile.write('wait='+str(waitTime)+'\n')
    # show disk usage info
    statusFile.write('disk.mntpt='+globalCfg['topDir']+'\n')
    try:
        (total, used, free) = disk_usage(globalCfg['topDir'])
        statusFile.write('disk.mounted=true\n')
        statusFile.write('disk.total.bytes='+str(total)+'\n')
        statusFile.write('disk.used.bytes='+str(used)+'\n')
        statusFile.write('disk.free.bytes='+str(free)+'\n')
        statusFile.write('disk.used.percent='+("%.2f" % (100.0*float(used)/float(total)))+'\n')
        statusFile.write('disk.free.percent='+("%.2f" % (100.0*float(free)/float(total)))+'\n')
    except OSError:
        statusFile.write('disk.mounted=false\n')
    statusFile.close()

    # CREATE A FORMATTED TABLE

    if (jobInfo == None):
        table = ['NOT YET STARTED']
    else:
        table = formattedTable(jobInfo)
    date = datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S')

    # DUMP THE SORTED LIST INTO THE QUEUE FILE

    queueFileName = libdir+'/queue'
    queueFile = open(queueFileName+suffix,'w')
    queueFile.write(date+' : '+activity+'\n')
    queueFile.write('\n')
    for line in table:
        queueFile.write(line+'\n')
    queueFile.close()

    # ATOMIC WRITE

    os.rename(statusFileName+suffix, statusFileName)
    os.rename(queueFileName+suffix, queueFileName)

    # SHOW THE SORTED LIST IN THE LOG

    log_info(date+' : '+activity)
    for line in table:
        log_info(line)
    log_info('')

#-----------------------------------------------------------

def do_rsync_backup(job):

    # if this is our first time, create a host/volume directory
    mkdir_p(globalCfg['topDir']+'/'+job['host']+'/'+job['volume'])

    args = [
        '-al',
        '-E',
        '-e', 'ssh -o PasswordAuthentication=no',
        '--delete',
        '--delete-excluded',
        '--numeric-ids',
        '--one-file-system',
        '--link-dest='+globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.1',
    ]
    src = job['src']
    dest = globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.0'

    # optional - "excludes" file
    excludes = globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/excludes'
    log_debug('testing for ['+excludes+']')
    if os.path.isfile(excludes):
        log_debug('"excludes" file found, adding argument')
        args.append('--exclude-from='+excludes)

    if len(job['rsyncOpts']) > 0:
        log_debug('"rsyncOpts" found, adding user arguments')
        args.extend(job['rsyncOpts'].split(' '))

    cmd = ['/usr/bin/rsync'] + args + [src, dest]
    (rc,stdout,stderr) = shell_capture(cmd)

    #   0      Success
    #   1      Syntax or usage error
    #   2      Protocol incompatibility
    #   3      Errors selecting input/output files, dirs
    #   4      Requested action not supported
    #   5      Error starting client-server protocol
    #   6      Daemon unable to append to log-file
    #   10     Error in socket I/O
    #   11     Error in file I/O
    #   12     Error in rsync protocol data stream
    #   13     Errors with program diagnostics
    #   14     Error in IPC code
    #   20     Received SIGUSR1 or SIGINT
    #   21     Some error returned by waitpid()
    #   22     Error allocating core memory buffers
    #   23     Partial transfer due to error
    #   24     Partial transfer due to vanished source files
    #   25     The --max-delete limit stopped deletions
    #   30     Timeout in data send/receive
    #   35     Timeout waiting for daemon connection
    complete = True if rc in (0, 24) else False
    log_debug('rc = %d'%rc + ', %s'%('OK' if complete else 'BAD') )

    if complete:
        prefix = globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
        if os.path.isdir(prefix+'0'):
            # "touch" the timestamp
            os.utime(prefix+'0',None)
        else:
            log_debug(prefix+'0 directory was not found, marking incomplete')
            complete = False
    else:
        log_debug('stdout:')
        log_debug(stdout)
        log_debug('stderr:')
        log_debug(stderr)

    log_debug('backup of '+job['host']+'/'+job['volume']+' is %s' %
        ('complete' if complete else 'incomplete') )

    return complete

#-----------------------------------------------------------

def do_cp_backup(job):

    # if this is our first time, create a host/volume directory
    mkdir_p(globalCfg['topDir']+'/'+job['host']+'/'+job['volume'])

    args = ['-alf']
    src = job['src']
    dest = globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.0'

    cmd = ['/bin/cp'] + args + [src, dest]
    (rc,stdout,stderr) = shell_capture(cmd)
    #   0      Success
    #   1      Failure
    complete = True if rc ==0 else False
    log_debug('rc = %d'%rc + ', %s'%('OK' if complete else 'BAD') )

    if complete:
        prefix = globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
        if os.path.isdir(prefix+'0'):
            # "touch" the timestamp
            os.utime(prefix+'0',None)
        else:
            log_debug(prefix+'0 directory was not found, marking incomplete')
            complete = False

    log_debug('backup of '+job['host']+'/'+job['volume']+' is %s' %
        ('complete' if complete else 'incomplete') )

    return complete

#-----------------------------------------------------------

def rotate(job):

    prefix = globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'

    # rotate the numbered backups
    if os.path.isdir(prefix+str(job['keepCount'])):
        log_debug('removing '+str(job['keepCount']))
        shutil.rmtree(prefix+str(job['keepCount']))
    rotates=[]
    for i in range(job['keepCount'],0,-1):
        if os.path.isdir(prefix+str(i-1)):
            os.rename(prefix+str(i-1),prefix+str(i))
            rotates.append(str(i-1)+'>>'+str(i))
    log_debug('rotating '+('  '.join(rotates)))

#-----------------------------------------------------------

def discard(job):

    prefix = globalCfg['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
    if os.path.isdir(prefix+'0'):
        log_debug('removing 0')
        shutil.rmtree(prefix+'0')

#-----------------------------------------------------------

def do_single_pass():

    # Server status is shown in status file, reflects what
    # the SERVER is doing, not the state of each backup job.
    serverIs = enum(
        IDLE       = 'IDLE',
        PREPARING  = 'PREPARING',
        BACKING_UP = 'BACKING_UP',
        ROTATING   = 'ROTATING',
        CLEANING   = 'CLEANING'
    )

    #write_status('thinking',0,None)
    jobs = buildJobTable()
    jobs = updateAgesAndSort(jobs)
    reportStatusAndQueue(serverIs.PREPARING, 'thinking...', None, 0, jobs)

    # GO THROUGH THE LIST IN ORDER, BACKING UP EACH ONE IF NEEDED

    log_info('start of single pass')
    for job in jobs:
        if job['ageSec'] < job['cycleSec'] : continue
        if job['disabled'] : continue

        # This is the name of our job, we'll use it a lot below.
        index = job['index']

        # First try to ping the host (full domain name) before trying to back it up.
        if ( ping(job['fqdn']) == False ):
            job['status'] = jobStatus.NOT_FOUND
            reportStatusAndQueue(serverIs.PREPARING, "'"+index+"' was not found", index, 0, jobs)
            continue

        # If we asked to probe the source directory first, look to see if the directory is there.
        if job['removable']:
            # check to see if the source is a directory (for removable media)
            (userAtHost,path) = job['src'].split(':')
            path = path.rstrip('/')
            cmd = ['/usr/bin/ssh', userAtHost, "bash -c 'if [[ -d "+path+" ]] ; then echo DIR ; fi'" ]
            (rc,stdout,stderr) = shell_capture(cmd)
            if stdout.strip() != 'DIR':
                log_info('job '+index+': probe failed, removable media not mounted')
                job['status'] = jobStatus.NOT_FOUND
                reportStatusAndQueue(serverIs.PREPARING, "'"+index+"' was not found", index, 0, jobs)
                continue
            log_info('job '+index+': probe succeeded, removable media is mounted')

        # Try to back up this job.
        job['status'] = jobStatus.BACKING_UP
        reportStatusAndQueue(serverIs.BACKING_UP, "backing up '"+index+"'", index, 0, jobs)
        startTime = datetime.datetime.now()
        if job['tool'] == 'rsync':
            if do_rsync_backup(job):
                job['status'] = jobStatus.ROTATING
                reportStatusAndQueue(serverIs.ROTATING, "rotating '"+index+"'", index, 0, jobs)
                rotate(job)
                job['status'] = jobStatus.SUCCEEDED
            else : # not complete
                job['status'] = jobStatus.FAILED
                reportStatusAndQueue(serverIs.CLEANING, "discarding '"+index+"'", index, 0, jobs)
                discard(job)
        elif job['tool'] == 'cp':
            if do_cp_backup(job):
                job['status'] = jobStatus.ROTATING
                reportStatusAndQueue(serverIs.ROTATING, "rotating '"+index+"'", index, 0, jobs)
                rotate(job)
                job['status'] = jobStatus.SUCCEEDED
            else : # not complete
                job['status'] = jobStatus.FAILED
                reportStatusAndQueue(serverIs.CLEANING, "discarding '"+index+"'", index, 0, jobs)
                discard(job)
        endTime = datetime.datetime.now()
        job['lastBackupDurationSec'] = (endTime - startTime).seconds
        log_debug('done')

    log_info('end of single pass')

    # sleep for a little bit
    for i in range(sleepMin,0,-1):
        jobs = updateAgesAndSort(jobs)
        reportStatusAndQueue(serverIs.IDLE, 'sleeping '+str(i)+' min', None, i, jobs)
        time.sleep(60)

#-----------------------------------------------------------

# START
def main():

    # FIRST -- PARSE COMMAND LINE
    usage = "usage: %prog [options]"
    parser = OptionParser(usage)
    parser.add_option("-d", "--debug", action="store_true", dest="debug")
    parser.add_option("-q", "--quiet", action="store_true", dest="quiet")
    global options
    (options, args) = parser.parse_args()
    if len(args) != 0:
        parser.error("incorrect number of arguments")

    # SET UP SERVICES

    log_init(logfile='/var/log/flashback-init.log')

    # LOOK FOR PID FILE, EXIT IF FOUND

    pidfile='/var/run/'+programName+'.pid'
    try:
        with open(pidfile) as f:
            log_info("pidfile '%s' found... better look for a running process" % pidfile)
            pid = int(f.readline())
            # Check For the existence of a unix pid, send signal 0 to it.
            try:
                os.kill(pid, 0)
            except OSError:
                log_info("pid %d not found, continuing" % pid)
            else:
                log_info("pid %d is still running, exiting" % pid)
                sys.exit()
    except IOError as e:
        pass
    f = open(pidfile,'w');
    f.write(str(os.getpid())+'\n')
    f.close()

    # READ GLOBAL OPTIONS

    globalConfigFile = '/etc/flashback.conf'
    if os.path.isfile(globalConfigFile):
        log_debug("reading global config file '"+globalConfigFile+"'")
        rawConfig = cfgfile_to_dict(globalConfigFile)
        # STRING OPTIONS
        setParmString(globalConfigFile,rawConfig,'topDir',globalCfg,'topDir')
        setParmString(globalConfigFile,rawConfig,'logFile',globalCfg,'logFile')
        setParmString(globalConfigFile,rawConfig,'label',globalCfg,'label')
        setParmString(globalConfigFile,rawConfig,'rsyncOpts',globalCfg,'rsyncOpts')
        setParmString(globalConfigFile,rawConfig,'tool',globalCfg,'tool')
        setParmString(globalConfigFile,rawConfig,'helper',globalCfg,'helper')
        # NUMBER OPTIONS
        setParmInt(globalConfigFile,rawConfig,'cycleDay',globalCfg,'cycleSec',86400)
        setParmInt(globalConfigFile,rawConfig,'cycleHour',globalCfg,'cycleSec',3600)
        setParmInt(globalConfigFile,rawConfig,'cycleMin',globalCfg,'cycleSec',60)
        setParmInt(globalConfigFile,rawConfig,'cycleSec',globalCfg,'cycleSec')
        setParmInt(globalConfigFile,rawConfig,'keepCount',globalCfg,'keepCount')

        if len(rawConfig) > 0:
            for key in rawConfig.keys():
                log_error("unknown parameter '"+key+"' in '"+globalConfigFile+"'")
            sys.exit(1)
    else:
        log_debug("no config file '"+globalConfigFile+"', using hard-coded globalCfg")

    # At this point, 'globalCfg' contains all config settings that are either
    # hard-coded at the top of this file, or overridden in /etc/flashback.conf.
    # They are still called "defaults" because each job can override (some of)
    # these values.

    # RUN HELPER SCRIPT, IF PRESENT

    # The user can specify a helper script to run before flashback gets
    # started.  This would be a good place to make sure we have a proper
    # IP address, set NTP time, mount external hard disks, etc.  If the
    # helper returns 0, then we're ready to start backing up stuff.
    reportStatusAndQueue('PRESTART', 'not yet started', None, 0, None)
    if 'helper' in globalCfg and os.path.isfile(globalCfg['helper']):
        cmd = [globalCfg['helper']]
        ## TODO: this hangs >> (rc,stdout,stderr) = shell_capture(cmd)
        rc = shell_do(cmd)
        if rc != 0:
            log_info("helper '"+globalCfg['helper']+"' failed with rc "+str(rc))
            sys.exit(1)

    # At this point, the large disk has been mounted.

    log_info('flashback initialized, further logs will be in '+globalCfg['logFile'])
    log_init(logfile=globalCfg['logFile'])
    log_info('START')
    log_debug("globalCfg : "+str(globalCfg)+"")

    # SANITY CHECK

    if os.path.isdir(globalCfg['topDir']) == False:
        log_debug("top level backup directory '"+globalCfg['topDir']+"' not found")
        sys.exit(1)

    # SET UP SUPPORT/STATUS DIRECTORY

    mkdir_p(libdir)

    # LOOP FOREVER, WORK AND SLEEP

    while True:
        do_single_pass()

    # CLEAN UP

    log_info('cleaning up')
    os.unlink(pidfile)

#-----------------------------------------------------------

def signal_handler(signal, frame):
    log_debug('received TERM signal')
    reportStatusAndQueue('DOWN', 'stopped', None, 0, None)
    sys.exit(0)

#-----------------------------------------------------------

if __name__ == "__main__":
    signal.signal(signal.SIGTERM, signal_handler)
    main()


