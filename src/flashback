#!/usr/bin/python -B

"""
TODO LIST

NOW
~~~
[x] snazzy name = flashback (was greenback, was sheevabackup)
[x] config files (global, per machine, per share)
[x] use rsync instead of rsback
[x] rotate backups myself
[x] plug-in to allow LED blinking
[x] store status in a file
[x] debian package
[x] push to github
[x] second-level backups
[x] top-level config file - change from import to config format
[x] top-level config file - move to /etc
[x] config file - allow comments
[x] config file - user-defined "topdir"
[x] config file - default cycle times, labels
[ ] write summary info to a per-backup status/history file (sqlite?)
[ ] bug?? : failures (half-copies) still update daily.1 timestamp
[ ] debian package installs monitor.sh in examples
[ ] test start/stop/install/uninstall
[x] rsyncOpts does not work
[x] when ping fails, set status to NOTFOUND, not FAILED
[ ] document - main config file options, overriding options in specific sub-config files
[ ] document - how to install (if you're not on a debian system)
consider ssh options >> StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null
consider client keys using >> from="sheeva.home.alanporter.com",no-pty

LATER
~~~~~
deduplication - audit runs after backup, compares "new" and "large" files to yesterday's
   large files, compares md5, creates hard links
"""

import os
import datetime
import subprocess
import shlex
import glob
import operator
import sys
from optparse import OptionParser
import time
import shutil
import imp
import errno

#-----------------------------------------------------------
#  H E L P E R   C L A S S E S
#-----------------------------------------------------------

def enum(**enums):
    return type('Enum', (), enums)

#-----------------------------------------------------------
#  S T A R T
#-----------------------------------------------------------

# globals
programName = 'flashback'
libdir = '/var/lib/'+programName
maxAge = 10000000
sleepMin = 10
# defaults (global)
defaults = {}
defaults['topDir'] = '/flashback'
defaults['lastBackupTimestamp'] = datetime.datetime(1970,1,1)
defaults['cycleSec'] = 24*60*60
defaults['keepCount'] = 9
defaults['label'] = 'daily'
defaults['tool'] = 'rsync'
defaults['rsyncOpts'] = ''
# plumbing (globals)
options = ()
g_logFD = None

jobStatus = enum(
    UNKNOWN    = 'UNKNOWN',
    NOT_READY  = 'NOT_READY',
    READY      = 'READY',
    NOT_FOUND  = 'NOT_FOUND',
    BACKING_UP = 'BACKING_UP',
    ROTATING   = 'ROTATING',
    SUCCEEDED  = 'SUCCEEDED',
    FAILED     = 'FAILED',
    DISABLED   = 'DISABLED'
)

#-----------------------------------------------------------

def jobfile_to_array(filename):
    try:
        log_debug("job file '"+filename+"' found")
        configFilePtr = open( filename, "r" )
    except OSError:
        log_debug("job file '"+filename+"' not found")
        sys.exit(1)

    # MAKE BASIC ENTRIES IN THE 'jobInfo' DICTIONARY FROM /etc/flashback.jobs

    sanitizedJobs = []
    for line in configFilePtr:
        line = line.partition('#')[0]  # strip comments
        line = line.rstrip('\r\n')     # strip trailing CR/LF
        line = line.strip(' ')         # strip leading space
        log_debug(filename+": "+line)
        pieces = line.split(' ')
        rawJob = {}   # raw junk read from config file
        for piece in pieces:
            keyValuePair = piece.split('=')
            if len(keyValuePair) == 2:
                key = keyValuePair[0]
                value = keyValuePair[1]
                rawJob[key] = value
        if len(rawJob) == 0:
            continue
        log_debug('raw job: '+','.join(['%s>%s' % (key, value) for (key, value) in rawJob.items()]) )

        sanitizedJob = {}   # sanitized job description
        # STRING OPTIONS
        req1 = setParmString(filename,rawJob,'host',sanitizedJob,'host')
        req2 = setParmString(filename,rawJob,'volume',sanitizedJob,'volume')
        req3 = setParmString(filename,rawJob,'src',sanitizedJob,'src')
        setParmString(filename,rawJob,'label',sanitizedJob,'label')
        setParmString(filename,rawJob,'rsyncOpts',sanitizedJob,'rsyncOpts')
        setParmString(filename,rawJob,'tool',sanitizedJob,'tool')
        # NUMBER OPTIONS
        setParmInt(filename,rawJob,'cycleDay',sanitizedJob,'cycleSec',86400)
        setParmInt(filename,rawJob,'cycleHour',sanitizedJob,'cycleSec',3600)
        setParmInt(filename,rawJob,'cycleMin',sanitizedJob,'cycleSec',60)
        setParmInt(filename,rawJob,'cycleSec',sanitizedJob,'cycleSec')
        setParmInt(filename,rawJob,'keepCount',sanitizedJob,'keepCount')
        # BOOLEAN OPTIONS
        setParmBool(filename,rawJob,'disabled',sanitizedJob,'disabled')

        # IF THE JOB INFO IS COMPLETE, THEN SAVE IT
        if len(sanitizedJob) > 0:
            if req1 and req2 and req3:
                log_debug('sanitizedJob:'+','.join(['%s>%s' % (key, value) for (key, value) in sanitizedJob.items()]) )
                sanitizedJobs.append(sanitizedJob)
            else:
                log_error('incomplete job '+','.join(['%s>%s' % (key, value) for (key, value) in sanitizedJob.items()]) )

        # IF THE JOB CONTAINS ERRORS, THEN STOP
        if len(rawJob) > 0:
            for key in rawJob.keys():
                log_error("unknown job parameter '"+key+"' in '"+filename+"'")
            sys.exit(1)

    log_debug('end of '+filename)
    return sanitizedJobs

#-----------------------------------------------------------

def cfgfile_to_dict(filename):
    try:
        log_debug("config file '"+filename+"' found")
        handle = open( filename, "r" )
    except OSError:
        log_debug("config file '"+filename+"' not found")
        return dict()

    # PARSE CONFIG FILE, BUILD DICTIONARY

    raw = {}
    for line in handle:
        line = line.partition('#')[0]  # strip comments
        line = line.rstrip('\r\n')     # strip trailing CR/LF
        line = line.strip(' ')         # strip leading space
        if len(line) == 0:
            continue
        if line.find('=') == -1:
            log_error('syntax error in config file '+filename+' > '+line)
            sys.exit(1)
        log_debug(filename+": "+line)
        (key,eq,value) = line.partition('=')
        key = key.strip()
        value = value.strip()
        raw[key] = value
        log_debug(filename+" "+key+":"+value)

    handle.close()
    log_debug('raw '+','.join(['%s:%s' % (key, value) for (key, value) in raw.items()]) )
    return raw

#-----------------------------------------------------------

def log_init():
    # we're writing these globals
    global g_logFD
    # log file
    #  logdir = os.environ['HOME']+"/var/log"
    #  if not os.path.exists(logdir):
    #     os.makedirs(logdir)
    #  logfile = logdir+"/garage.log"
    #  logfile = programName+'.log'
    logfile = '/dev/stdout'
    g_logFD = open(logfile,'w')

#-----------------------------------------------------------

def log_debug(string):
    if options.debug:
        log_info(string)

#-----------------------------------------------------------

def log_info(string):
    if options.quiet:
        return
    global g_logFD
    timeStamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    g_logFD.write(timeStamp+" "+string+"\n")
    g_logFD.flush()
    ##os.fsync(g_logFD)

#-----------------------------------------------------------

def log_error(string):
    global g_logFD
    timeStamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    g_logFD.write(timeStamp+" ERROR "+string+"\n")
    g_logFD.flush()
    ##os.fsync(g_logFD)

#-----------------------------------------------------------

def shell_capture(cmdargs):
    global g_logFD
    log_debug('shell_capture command >> '+(' '.join(cmdargs)))
    p = subprocess.Popen(cmdargs, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = p.communicate()
    rc = p.returncode
    log_debug("shell_capture done, rc="+('%d'%rc))
    return rc, stdout, stderr

#-----------------------------------------------------------

def shell_do(cmdargs):
    global g_logFD
    log_debug('shell_do command >> '+(' '.join(cmdargs)))
    rc = subprocess.call(cmdargs)
    #rc = subprocess.call(cmdargs, stdout=g_logFD, stderr=g_logFD)
    log_debug("shell_do rc = "+("%d"%rc))
    return rc

#-----------------------------------------------------------

def sec2dhms(s):
    days = s // 86400  ; s = s - (days * 86400)
    hours = s // 3600  ; s = s - (hours * 3600)
    mins = s // 60     ; s = s - (mins * 60)
    secs = s
    return (days, hours, mins, secs)

#-----------------------------------------------------------

def ping(host):
    return True if ( shell_do(['ping','-c1',host]) == 0 ) else False

#-----------------------------------------------------------

def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc : # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

#-----------------------------------------------------------

def disk_usage(path):
    """Return disk usage statistics about the given path.

    Returned value is a tuple with three attributes:
    'total', 'used' and 'free' (in bytes).
    """
    st = os.statvfs(path)
    free = st.f_bavail * st.f_frsize
    total = st.f_blocks * st.f_frsize
    used = (st.f_blocks - st.f_bfree) * st.f_frsize
    return (total, used, free)

#-----------------------------------------------------------

def setParmString(cfgFile,cfgHash,cfgIdx,volHash,volIdx):
    if cfgIdx not in cfgHash : return False
    volHash[volIdx] = cfgHash[cfgIdx]
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> STRING >> '+volIdx+'='+volHash[volIdx])
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def setParmInt(cfgFile,cfgHash,cfgIdx,volHash,volIdx,multiplier=1):
    if cfgIdx not in cfgHash : return False
    volHash[volIdx] = int(cfgHash[cfgIdx]) * multiplier
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> INT >> '+volIdx+'='+str(volHash[volIdx]))
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def setParmBool(cfgFile,cfgHash,cfgIdx,volHash,volIdx):
    if cfgIdx not in cfgHash : return False
    if cfgHash[cfgIdx].lower() in ['yes','y','true','1']:
        volHash[volIdx] = True
    else:
        volHash[volIdx] = False
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> BOOL >> '+volIdx+'='+( 'TRUE' if volHash[volIdx] else 'FALSE'))
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def buildJobTable():

    jobInfo = jobfile_to_array('/etc/flashback.jobs')

    # FILL IN THE 'jobInfo' DICTIONARY FROM DEFAULTS AND MORE CONFIGS

    for job in jobInfo:

        log_debug('job:'+','.join(['%s>%s' % (key, value) for (key, value) in job.items()]) )

        # set some defaults - always
        job['status'] = jobStatus.UNKNOWN
        job['lastBackupDurationSec'] = 0
        # set some defaults - only if not set in main jobs config file (TODO: change this ???)
        for i in ['label','cycleSec','keepCount','tool','rsyncOpts']:
            if i not in job : job[i] = defaults[i]
        if 'disabled' not in job : job['disabled'] = False

        # set an index to refer to this entry by
        index = job['host']+'-'+job['volume']+'-'+job['label']
        job['index'] = index

        # build a list of several optional config files to look for
        configFilenames=[]
        configFilenames.append(defaults['topDir']+'/config') # /backup/config
        configFilenames.append(defaults['topDir']+'/'+job['host']+'/config') # /backup/host/config
        configFilenames.append(defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/config') # /backup/host/volume/config

        for configFilename in configFilenames:
            # read the config files if they exist
            if os.path.isfile(configFilename):
                log_debug("reading config file '"+configFilename+"'")
                rawConfig = cfgfile_to_dict(configFilename)
                # STRING OPTIONS
                setParmString(configFilename,rawConfig,'src',job,'src')
                setParmString(configFilename,rawConfig,'label',job,'label')
                setParmString(configFilename,rawConfig,'rsyncOpts',job,'rsyncOpts')
                setParmString(configFilename,rawConfig,'tool',job,'tool')
                # NUMBER OPTIONS
                setParmInt(configFilename,rawConfig,'cycleDay',job,'cycleSec',86400)
                setParmInt(configFilename,rawConfig,'cycleHour',job,'cycleSec',3600)
                setParmInt(configFilename,rawConfig,'cycleMin',job,'cycleSec',60)
                setParmInt(configFilename,rawConfig,'cycleSec',job,'cycleSec')
                setParmInt(configFilename,rawConfig,'keepCount',job,'keepCount')
                # BOOLEAN OPTIONS
                setParmBool(configFilename,rawConfig,'disabled',job,'disabled')

                if len(rawConfig) > 0:
                    for key in rawConfig.keys():
                        log_error("unknown parameter '"+key+"' in '"+configFilename+"'")
                    sys.exit(1)

            else:
                log_debug("no config file '"+configFilename+"'")

        # Get the creation time of the daily.1 directory, if it exists.
        recentBackup = defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.1'
        # Note: ctime() does not refer to creation time on *nix systems,
        # but rather the last time the inode data changed.
        if os.path.exists(recentBackup):
            mtime = os.path.getmtime(recentBackup)
            job['lastBackupTimestamp'] = datetime.datetime.fromtimestamp(mtime)
        else:
            job['lastBackupTimestamp'] = defaults['lastBackupTimestamp']

    return jobInfo

#-----------------------------------------------------------

def updateAgesAndSort(jobInfo):

    # GO THROUGH THE LIST IN ORDER, DETERMINE THEIR AGES AND NEXT BACKUP TIME

    now = datetime.datetime.now()
    for job in jobInfo:
        ageDelta = now - job['lastBackupTimestamp']
        job['ageSec'] = ageDelta.seconds + (ageDelta.days * 86400)
        job['nextBackupSec'] = job['cycleSec'] - job['ageSec']
        log_debug('index='+job['index']+', nextBackupSec='+str(job['nextBackupSec'])+'sec')
        # force disabled backups to the bottom of the list
        if job['disabled'] : job['nextBackupSec'] = maxAge

    for job in jobInfo:
        if job['status'] in (jobStatus.NOT_READY, jobStatus.UNKNOWN):
            # If "cycleSec" has transpired since our last backup, we're "ready".
            if job['ageSec'] > job['cycleSec'] : job['status'] = jobStatus.READY
            else : job['status'] = jobStatus.NOT_READY
        # No matter if "ready" or not, if disabled, don't back up.
        if job['disabled']: job['status'] = jobStatus.DISABLED

    sortedJobs = sorted(jobInfo, key=operator.itemgetter('nextBackupSec'))

    return sortedJobs

#-----------------------------------------------------------

def formattedTable(jobs):
    widths = { 'index':0, 'lastBackupTimestamp':0, 'ageSec':0, 'cycleSec':0 }
    for job in jobs:
        for fld in ('index','ageSec','cycleSec'):
            widths[fld] = max(widths[fld], len(str(job[fld])))
    #widths['lastBackupTimestamp'] = len('| 2013-03-20 21:05:18 | 1368/1440  |  NEXT RUN 0d+0:01:12')
    widths['lastBackupTimestamp'] = len('2013-03-20 21:05:18')

    table = []
    table.append(''
        + 'INDEX'.center(widths['index'])                     + '   '
        + 'LAST BACKUP'.center(widths['lastBackupTimestamp']) + '   '
        + 'AGE'.center(widths['ageSec'])                      + '/'
        + 'CYCLE'.center(widths['cycleSec'])                  + '   '
        + 'STATUS')

    now = datetime.datetime.now()
    for job in jobs:
        (d,h,m,s) = sec2dhms( job['nextBackupSec'] )
        nextBackupInterval = '%dd+%d:%02d:%02d' % (d,h,m,s)
        (d,h,m,s) = sec2dhms( job['lastBackupDurationSec'] )  # assume 0 days
        lastBackupDurationSec = '%d:%02d:%02d' % (h,m,s)
        switch = {
            jobStatus.UNKNOWN :    '???',
            jobStatus.DISABLED :   'DISABLED',
            jobStatus.NOT_READY :  'NEXT RUN IN '+nextBackupInterval,
            jobStatus.NOT_FOUND :  'NOT FOUND',
            jobStatus.BACKING_UP : 'BACKING UP',
            jobStatus.ROTATING :   'ROTATING',
            jobStatus.READY :      'READY',
            jobStatus.SUCCEEDED :  'SUCCEEDED ('+lastBackupDurationSec+')',
            jobStatus.FAILED :     'FAILED ('+lastBackupDurationSec+')',
        }
        lastBackupTimestampString = job['lastBackupTimestamp'].strftime('%Y-%m-%d %H:%M:%S')
        table.append(''
            + job['index'].ljust(widths['index'])            + '   '
            + lastBackupTimestampString                      + '   '
            + str(job['ageSec']).rjust(widths['ageSec'])     + '/'
            + str(job['cycleSec']).ljust(widths['cycleSec']) + '   '
            + switch[job['status']]
        )
    return table

#-----------------------------------------------------------

def reportStatusAndQueue(health,activity,target,waitTime,jobInfo):

    log_debug('writing status file and queue file')
    suffix = str(os.getpid())

    # CREATE A SMALL STATUS FILE

    statusFileName = libdir+'/status'
    statusFile = open(statusFileName+suffix,'w')
    now = datetime.datetime.now()
    # show time, date and PID - so we know that the info is current
    statusFile.write('date='+datetime.datetime.strftime(now,'%Y-%m-%d')+'\n')
    statusFile.write('time='+datetime.datetime.strftime(now,'%H:%M:%S')+'\n')
    statusFile.write('pid='+str(os.getpid())+'\n')
    # show what we're doing (a verb)
    statusFile.write('status='+health+'\n')
    # show what we're doing it to (an object)
    if target is None : target=''
    statusFile.write('target='+target+'\n')
    # show how long we'll do it (only if we're sleeping)
    statusFile.write('wait='+str(waitTime)+'\n')
    # show disk usage info
    (total, used, free) = disk_usage(defaults['topDir'])
    statusFile.write('disk.mntpt='+defaults['topDir']+'\n')
    statusFile.write('disk.total.bytes='+str(total)+'\n')
    statusFile.write('disk.used.bytes='+str(used)+'\n')
    statusFile.write('disk.free.bytes='+str(free)+'\n')
    statusFile.write('disk.used.percent='+("%.2f" % (100.0*float(used)/float(total)))+'\n')
    statusFile.write('disk.free.percent='+("%.2f" % (100.0*float(free)/float(total)))+'\n')
    statusFile.close()

    # CREATE A FORMATTED TABLE

    if (jobInfo == None):
        table = ['NOT YET STARTED']
    else:
        table = formattedTable(jobInfo)
    date = datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S')

    # DUMP THE SORTED LIST INTO THE QUEUE FILE

    queueFileName = libdir+'/queue'
    queueFile = open(queueFileName+suffix,'w')
    queueFile.write(date+' : '+activity+'\n')
    queueFile.write('\n')
    for line in table:
        queueFile.write(line+'\n')
    queueFile.close()

    # ATOMIC WRITE

    os.rename(statusFileName+suffix, statusFileName)
    os.rename(queueFileName+suffix, queueFileName)

    # SHOW THE SORTED LIST IN THE LOG

    log_info(date+' : '+activity)
    for line in table:
        log_info(line)
    log_info('')

#-----------------------------------------------------------

def do_rsync_backup(job):

    # if this is our first time, create a host/volume directory
    mkdir_p(defaults['topDir']+'/'+job['host']+'/'+job['volume'])

    args = [
        '-al',
        '-E',
        '-e', 'ssh -o PasswordAuthentication=no',
        '--delete',
        '--delete-excluded',
        '--numeric-ids',
        '--one-file-system',
#       '-v',
#       '--stats',
        '--link-dest='+defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.1',
    ]
    src = job['src']
    dest = defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.0'

    # optional - "excludes" file
    excludes = defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/excludes'
    log_debug('testing for ['+excludes+']')
    if os.path.isfile(excludes):
        log_debug('"excludes" file found, adding argument')
        args.append('--exclude-from='+excludes)

    if len(job['rsyncOpts']) > 0:
        log_debug('"rsyncOpts" found, adding user arguments')
        args.extend(job['rsyncOpts'].split(' '))

    cmd = ['/usr/bin/rsync'] + args + [src, dest]
    log_info('running >> '+(','.join(cmd)))
    (rc,stdout,stderr) = shell_capture(cmd)

    #   0      Success
    #   1      Syntax or usage error
    #   2      Protocol incompatibility
    #   3      Errors selecting input/output files, dirs
    #   4      Requested action not supported
    #   5      Error starting client-server protocol
    #   6      Daemon unable to append to log-file
    #   10     Error in socket I/O
    #   11     Error in file I/O
    #   12     Error in rsync protocol data stream
    #   13     Errors with program diagnostics
    #   14     Error in IPC code
    #   20     Received SIGUSR1 or SIGINT
    #   21     Some error returned by waitpid()
    #   22     Error allocating core memory buffers
    #   23     Partial transfer due to error
    #   24     Partial transfer due to vanished source files
    #   25     The --max-delete limit stopped deletions
    #   30     Timeout in data send/receive
    #   35     Timeout waiting for daemon connection
    complete = True if rc in (0, 24) else False
    log_debug('rc = %d'%rc + ', %s'%('OK' if complete else 'BAD') )

    if complete:
        prefix = defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
        if os.path.isdir(prefix+'0'):
            # "touch" the timestamp
            os.utime(prefix+'0',None)
        else:
            log_debug(prefix+'0 directory was not found, marking incomplete')
            complete = False

    log_debug('backup of '+job['host']+'/'+job['volume']+' is %s' %
        ('complete' if complete else 'incomplete') )

    return complete

#-----------------------------------------------------------

def do_cp_backup(job):

    # if this is our first time, create a host/volume directory
    mkdir_p(defaults['topDir']+'/'+job['host']+'/'+job['volume'])

    args = ['-alf']
    src = job['src']
    dest = defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.0'

    cmd = ['/bin/cp'] + args + [src, dest]
    log_info('running >> '+(','.join(cmd)))
    (rc,stdout,stderr) = shell_capture(cmd)
    #   0      Success
    #   1      Failure
    complete = True if rc ==0 else False
    log_debug('rc = %d'%rc + ', %s'%('OK' if complete else 'BAD') )

    if complete:
        prefix = defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
        if os.path.isdir(prefix+'0'):
            # "touch" the timestamp
            os.utime(prefix+'0',None)
        else:
            log_debug(prefix+'0 directory was not found, marking incomplete')
            complete = False

    log_debug('backup of '+job['host']+'/'+job['volume']+' is %s' %
        ('complete' if complete else 'incomplete') )

    return complete

#-----------------------------------------------------------

def rotate(job):

    prefix = defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'

    # rotate the numbered backups
    if os.path.isdir(prefix+str(job['keepCount'])):
        log_debug('removing '+str(job['keepCount']))
        shutil.rmtree(prefix+str(job['keepCount']))
    rotates=[]
    for i in range(job['keepCount'],0,-1):
        if os.path.isdir(prefix+str(i-1)):
            os.rename(prefix+str(i-1),prefix+str(i))
            rotates.append(str(i-1)+'>>'+str(i))
    log_debug('rotating '+('  '.join(rotates)))

#-----------------------------------------------------------

def discard(job):

    prefix = defaults['topDir']+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
    if os.path.isdir(prefix+'0'):
        log_debug('removing 0')
        shutil.rmtree(prefix+'0')

#-----------------------------------------------------------

def do_single_pass():

    # Server status is shown in status file, reflects what
    # the SERVER is doing, not the state of each backup job.
    serverIs = enum(
        IDLE       = 'IDLE',
        PREPARING  = 'PREPARING',
        BACKING_UP = 'BACKING_UP',
        ROTATING   = 'ROTATING',
        CLEANING   = 'CLEANING'
    )

    #write_status('thinking',0,None)
    jobs = buildJobTable()
    jobs = updateAgesAndSort(jobs)
    reportStatusAndQueue(serverIs.PREPARING, 'thinking...', None, 0, jobs)

    # GO THROUGH THE LIST IN ORDER, BACKING UP EACH ONE IF NEEDED

    log_info('start of single pass')
    for job in jobs:
        if job['ageSec'] < job['cycleSec'] : continue
        if job['disabled'] : continue
        # This is the name of our job, we'll use it a lot below.
        index = job['index']
        # First try to ping the host before trying to back it up.
        if ( ping(job['host']) == False ):
            job['status'] = jobStatus.NOT_FOUND
            reportStatusAndQueue(serverIs.PREPARING, "'"+index+"' was not found", index, 0, jobs)
            continue
        # Try to back up this job.
        job['status'] = jobStatus.BACKING_UP
        reportStatusAndQueue(serverIs.BACKING_UP, "backing up '"+index+"'", index, 0, jobs)
        startTime = datetime.datetime.now()
        if job['tool'] == 'rsync':
            if do_rsync_backup(job):
                job['status'] = jobStatus.ROTATING
                reportStatusAndQueue(serverIs.ROTATING, "rotating '"+index+"'", index, 0, jobs)
                rotate(job)
                job['status'] = jobStatus.SUCCEEDED
            else : # not complete
                job['status'] = jobStatus.FAILED
                reportStatusAndQueue(serverIs.CLEANING, "discarding '"+index+"'", index, 0, jobs)
                discard(job)
        elif job['tool'] == 'cp':
            if do_cp_backup(job):
                job['status'] = jobStatus.ROTATING
                reportStatusAndQueue(serverIs.ROTATING, "rotating '"+index+"'", index, 0, jobs)
                rotate(job)
                job['status'] = jobStatus.SUCCEEDED
            else : # not complete
                job['status'] = jobStatus.FAILED
                reportStatusAndQueue(serverIs.CLEANING, "discarding '"+index+"'", index, 0, jobs)
                discard(job)
        endTime = datetime.datetime.now()
        job['lastBackupDurationSec'] = (endTime - startTime).seconds
        log_debug('done')

    log_info('end of single pass')

    # sleep for a little bit
    for i in range(sleepMin,0,-1):
        jobs = updateAgesAndSort(jobs)
        reportStatusAndQueue(serverIs.IDLE, 'sleeping '+str(i)+' min', None, i, jobs)
        time.sleep(60)

#-----------------------------------------------------------

# START
def main():

    # FIRST -- PARSE COMMAND LINE
    usage = "usage: %prog [options]"
    parser = OptionParser(usage)
    parser.add_option("-d", "--debug", action="store_true", dest="debug")
    parser.add_option("-q", "--quiet", action="store_true", dest="quiet")
    global options
    (options, args) = parser.parse_args()
    if len(args) != 0:
        parser.error("incorrect number of arguments")

    # SET UP SERVICES

    log_init()

    # LOOK FOR PID FILE, EXIT IF FOUND

    pidfile='/var/run/'+programName+'.pid'
    try:
        with open(pidfile) as f:
            log_info("pidfile '%s' found... better look for a running process" % pidfile)
            pid = int(f.readline())
            # Check For the existence of a unix pid, send signal 0 to it.
            try:
                os.kill(pid, 0)
            except OSError:
                log_info("pid %d not found, continuing" % pid)
            else:
                log_info("pid %d is still running, exiting" % pid)
                sys.exit()
    except IOError as e:
        pass
    f = open(pidfile,'w');
    f.write(str(os.getpid())+'\n')
    f.close()

    # READ GLOBAL OPTIONS

    globalConfig = '/etc/flashback.conf'
    if os.path.isfile(globalConfig):
        log_debug("reading global config file '"+globalConfig+"'")
        rawConfig = cfgfile_to_dict(globalConfig)
        # STRING OPTIONS
        setParmString(globalConfig,rawConfig,'topDir',defaults,'topDir')
        setParmString(globalConfig,rawConfig,'label',defaults,'label')
        setParmString(globalConfig,rawConfig,'rsyncOpts',defaults,'rsyncOpts')
        setParmString(globalConfig,rawConfig,'tool',defaults,'tool')
        setParmString(globalConfig,rawConfig,'preScript',defaults,'preScript')
        # NUMBER OPTIONS
        setParmInt(globalConfig,rawConfig,'cycleDay',defaults,'cycleSec',86400)
        setParmInt(globalConfig,rawConfig,'cycleHour',defaults,'cycleSec',3600)
        setParmInt(globalConfig,rawConfig,'cycleMin',defaults,'cycleSec',60)
        setParmInt(globalConfig,rawConfig,'cycleSec',defaults,'cycleSec')
        setParmInt(globalConfig,rawConfig,'keepCount',defaults,'keepCount')

        if len(rawConfig) > 0:
            for key in rawConfig.keys():
                log_error("unknown parameter '"+key+"' in '"+globalConfig+"'")
            sys.exit(1)
    else:
        log_debug("no config file '"+globalConfig+"', using defaults")

    # SANITY CHECK

    if os.path.isdir(defaults['topDir']) == False:
        log_debug("top level backup directory '"+defaults['topDir']+"' not found")
        sys.exit(1)

    # SET UP SUPPORT/STATUS DIRECTORY

    mkdir_p(libdir)

    # RUN PRE-SCRIPT (may set time, mount disk, start monitor, etc)

    reportStatusAndQueue('PRESCRIPT', 'not yet started', None, 0, None)

    if os.path.isfile(defaults['preScript']):
        cmd = [defaults['preScript']]
        log_info('running >> '+(','.join(cmd)))
        (rc,stdout,stderr) = shell_capture(cmd)
        if rc != 0:
            log_debug("pre-script '"+defaults['preScript']+"' failed with rc "+str(rc))
            sys.exit(1)

    # LOOP FOREVER, WORK AND SLEEP

    while True:
        do_single_pass()

    # CLEAN UP

    log_info('cleaning up')
    os.unlink(pidfile)

#-----------------------------------------------------------

if __name__ == "__main__":
     main()


