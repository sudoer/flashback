#!/usr/bin/python -B

import argparse
import datetime
import errno
import glob
import json
import operator
import os
import shlex
import shutil
import signal
import subprocess
import sys
import time

#-----------------------------------------------------------
#  H E L P E R   C L A S S E S
#-----------------------------------------------------------

def enum(**enums):
    return type('Enum', (), enums)

#-----------------------------------------------------------
#  S T A R T
#-----------------------------------------------------------

# globals
PROGRAM_NAME = 'flashback'
maxAge = 10000000
sleepMin = 10

# globalCfg (global)
globalCfgFile = None
globalCfg = {}

# plumbing (globals)
options = ()
g_logFD = None

jobStatus = enum(
    UNKNOWN    = 'UNKNOWN',
    NOT_READY  = 'NOT_READY',
    READY      = 'READY',
    NOT_FOUND  = 'NOT_FOUND',
    BACKING_UP = 'BACKING_UP',
    ROTATING   = 'ROTATING',
    SUCCEEDED  = 'SUCCEEDED',
    FAILED     = 'FAILED',
    DISABLED   = 'DISABLED'
)
# Server status is shown in status file, reflects what
# the SERVER is doing, not the state of each backup job.
serverIs = enum(
    IDLE       = 'IDLE',
    PREPARING  = 'PREPARING',
    BACKING_UP = 'BACKING_UP',
    ROTATING   = 'ROTATING',
    CLEANING   = 'CLEANING'
)

#-----------------------------------------------------------

# configs

def read_config_file(config_file=None):
    global globalCfgFile
    global globalCfg

    globalCfgFile = config_file or globalCfgFile 
    globalCfg = {}
    if globalCfgFile:
        f = open(globalCfgFile)
        globalCfg = json.load(f)
        f.close()
    apply_config_defaults()

def apply_config_defaults():
    defaults = [
        ('dataDir', '/flashback'),
        ('libDir', f'/var/lib/{PROGRAM_NAME}'),
        ('pidDir', f'/var/run'),
        ('logFile', '/dev/stdout'),
        ('cycleSec', 24*60*60),
        ('keepCount', 9),
        ('label', 'daily'),
        ('tool', 'rsync'),
        ('rsyncOpts', ''),
        ('removable', False),
    ]
    for key, value in defaults:
        if key not in globalCfg['global']:
            globalCfg['global'][key] = value

def config_val(key, paths, default=None):
    for path in paths:
        cfg = globalCfg
        for part in path:
            if part in cfg:
                cfg = cfg[part]
            else:
                cfg = {}
        if key in cfg:
            log_debug(f'found key {key} in {".".join(path)} -> {cfg[key]}')
            return cfg[key]
    log_debug(f'using default for key {key} -> {default}')
    return default

#-----------------------------------------------------------

def log_init(logfile=None):
    # we're writing these globals
    global g_logFD
    # log file
    if g_logFD:
        g_logFD.close()
    if logfile is not None:
        g_logFD = open(logfile,'a')
    else:
        g_logFD = open('/dev/stderr','a')

def log_debug(string):
    if options.debug:
        log_info(string)

def log_info(string):
    if options.quiet:
        return
    timeStamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    g_logFD.write(timeStamp+" "+string+"\n")
    g_logFD.flush()
    ##os.fsync(g_logFD)

#-----------------------------------------------------------

def log_error(string):
    timeStamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    g_logFD.write(timeStamp+" ERROR "+string+"\n")
    g_logFD.flush()
    ##os.fsync(g_logFD)

#-----------------------------------------------------------

def format_arglist_into_shellcmd(originalArgs):
    copyOfArgs = list(originalArgs)
    for idx, arg in enumerate(copyOfArgs):
        if arg.find(' ') != -1:
            modifiedArg = '"'+arg.replace('"','\\"')+'"'
            copyOfArgs[idx] = modifiedArg
    return (' '.join(copyOfArgs))

#-----------------------------------------------------------

def shell_capture_2(cmdargs, poll_time, poll_func, poll_args, poll_kwargs, stdout=None, stderr=None):
    log_debug('shell_capture_2 arguments >> '+(",".join(cmdargs)))
    log_debug('shell_capture_2 command >> '+format_arglist_into_shellcmd(cmdargs))

    if options.no_run:
        return 0

    p = subprocess.Popen(cmdargs, stdout=stdout, stderr=stderr)
    # wait for process to finish
    while p.poll() is None:
        time.sleep(poll_time)
        poll_func(*poll_args, **poll_kwargs)
    rc = p.returncode
    log_debug("shell_capture_2 done, rc="+('%d'%rc))
    return rc

#-----------------------------------------------------------

def shell_capture(cmdargs):
    log_debug('shell_capture arguments >> '+(",".join(cmdargs)))
    log_debug('shell_capture command >> '+format_arglist_into_shellcmd(cmdargs))

    if options.no_run:
        return 0, "", ""

    p = subprocess.Popen(cmdargs, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    # wait for process to finish, capture stdout and stderr
    stdout, stderr = p.communicate()
    rc = p.returncode
    log_debug("shell_capture done, rc="+('%d'%rc))
    return rc, stdout, stderr

#-----------------------------------------------------------

def shell_do(cmdargs):
    log_debug('shell_do arguments >> '+(",".join(cmdargs)))
    log_debug('shell_do command >> '+format_arglist_into_shellcmd(cmdargs))

    if options.no_run:
        return 0

    rc = subprocess.call(cmdargs)
    log_debug("shell_do rc = "+("%d"%rc))
    return rc

#-----------------------------------------------------------

def sec2dhms(s):
    days = s // 86400  ; s = s - (days * 86400)
    hours = s // 3600  ; s = s - (hours * 3600)
    mins = s // 60     ; s = s - (mins * 60)
    secs = s
    return (days, hours, mins, secs)

#-----------------------------------------------------------

def sec2string(s):
    (d,h,m,s) = sec2dhms(s)
    if d > 0:
        return '%dd+%d:%02d:%02d' % (d,h,m,s)
    else:
        return '%d:%02d:%02d' % (h,m,s)

#-----------------------------------------------------------

def ping(host):
    return True if ( shell_do(['ping','-c1',host]) == 0 ) else False

#-----------------------------------------------------------

def mkdir_p(path):
    try:
        os.makedirs(path)
    except OSError as exc:
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else:
            raise

#-----------------------------------------------------------

def disk_usage(path):
    """Return disk usage statistics about the given path.

    Returned value is a tuple with three attributes:
    'total', 'used' and 'free' (in bytes).
    """
    st = os.statvfs(path)
    free = st.f_bavail * st.f_frsize
    total = st.f_blocks * st.f_frsize
    used = (st.f_blocks - st.f_bfree) * st.f_frsize
    return (total, used, free)

#-----------------------------------------------------------

def remove_files_recursive(directory):
    # (1) shutil.rmtree(directory)
    # (2) os.system('rm -fr "%s"' % directory)
    empty = "/tmp/empty"
    mkdir_p(empty)
    os.system('/usr/bin/rsync -a --delete "%s/" "%s/"' % (empty, directory))
    os.rmdir(empty)

#-----------------------------------------------------------

def setParmString(cfgFile,cfgHash,cfgIdx,volHash,volIdx):
    if cfgIdx not in cfgHash : return False
    volHash[volIdx] = cfgHash[cfgIdx]
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> STRING >> '+volIdx+'='+volHash[volIdx])
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def setParmInt(cfgFile,cfgHash,cfgIdx,volHash,volIdx,multiplier=1):
    if cfgIdx not in cfgHash : return False
    volHash[volIdx] = int(cfgHash[cfgIdx]) * multiplier
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> INT >> '+volIdx+'='+str(volHash[volIdx]))
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def setParmBool(cfgFile,cfgHash,cfgIdx,volHash,volIdx):
    if cfgIdx not in cfgHash : return False
    if cfgHash[cfgIdx].lower() in ['yes','y','true','1']:
        volHash[volIdx] = True
    else:
        volHash[volIdx] = False
    log_info(' - '+cfgFile+': '+cfgIdx+'='+cfgHash[cfgIdx]+' >> BOOL >> '+volIdx+'='+( 'TRUE' if volHash[volIdx] else 'FALSE'))
    del cfgHash[cfgIdx]
    return True

#-----------------------------------------------------------

def buildJobTable():
    """
    Read the config JSON file and build a list of job dictionaries.
    Each one has all of the parameters needed to run, as well as current status.
    """

    job_info_list = []

    cfg_hosts = config_val('hosts', [[]], {})

    for host in cfg_hosts:
        cfg_volumes = config_val('volumes', [['hosts', host]], {})
        for volume in cfg_volumes:
            cfg_jobs = config_val('jobs', [['hosts', host, 'volumes', volume]], {})
            for job in cfg_jobs:
                log_debug(f'found host={host}, volume={volume}, job={job}')
                search_path = [
                    ['hosts', host, 'volumes', volume, 'jobs', job],
                    ['hosts', host, 'volumes', volume],
                    ['hosts', host],
                    ['defaults'],
                ]
                # string options
                cfg_src = config_val('src', search_path)
                cfg_fqdn = config_val('fqdn', search_path)
                cfg_rsync_opts = config_val('rsyncOpts', search_path)
                cfg_tool = config_val('tool', search_path)
                # number options
                cfg_cycleSec = (
                    (config_val('cycleDay', search_path, 0) * 86400)
                    or (config_val('cycleHour', search_path, 0) * 3600)
                    or (config_val('cycleMin', search_path, 0) * 60)
                    or (config_val('cycleSec', search_path, 0) * 1)
                )
                cfg_keepCount = config_val('keepCount', search_path)
                # boolean options
                cfg_disabled = config_val('disabled', search_path, False)
                cfg_removable = config_val('removable', search_path, False)
                # list options
                cfg_excludes = config_val('excludes', search_path, [])

                # Specifying an FQDN is optional in the config file.
                if not cfg_fqdn:
                    cfg_fqdn = host
                # If host is in FQDN format (with dots), trim them out.
                if host.find('.') >= 0:
                    host = host.split('.')[0]

                # Get the creation time of the daily.1 directory, if it exists.
                cfg_datadir = config_val('dataDir', [['global']])
                recentBackup = cfg_datadir+'/'+host+'/'+volume+'/'+job+'.1'
                # Note: ctime() does not refer to creation time on *nix systems,
                # but rather the last time the inode data changed.
                if os.path.exists(recentBackup):
                    mtime = os.path.getmtime(recentBackup)
                    lastBackupTimestamp = datetime.datetime.fromtimestamp(mtime)
                else:
                    lastBackupTimestamp = datetime.datetime(1970,1,1)

                job_info = {
                    'index': f"{host}-{volume}-{job}",
                    'status': jobStatus.UNKNOWN,
                    'host': host,
                    'volume': volume,
                    'label': job,
                    'src': cfg_src,
                    'fqdn': cfg_fqdn,
                    'rsyncOpts': cfg_rsync_opts,
                    'excludes': cfg_excludes,
                    'tool': cfg_tool,
                    'cycleSec': cfg_cycleSec,
                    'keepCount': cfg_keepCount,
                    'disabled': cfg_disabled,
                    'removable': cfg_removable,
                    'lastBackupTimestamp': lastBackupTimestamp,
                    'lastBackupDurationSec': 0,
                }
                job_info_list.append(job_info)

    return job_info_list

#-----------------------------------------------------------

def updateAgesAndSort(jobInfo):

    # GO THROUGH THE LIST IN ORDER, DETERMINE THEIR AGES AND NEXT BACKUP TIME

    now = datetime.datetime.now()
    for job in jobInfo:
        ageDelta = now - job['lastBackupTimestamp']
        job['ageSec'] = ageDelta.seconds + (ageDelta.days * 86400)
        log_debug(f"job['cycleSec'] = {job['cycleSec']} and job['ageSec'] = {job['ageSec']}")
        job['nextBackupSec'] = job['cycleSec'] - job['ageSec']
        log_debug('index='+job['index']+', nextBackupSec='+str(job['nextBackupSec'])+'sec')
        # force disabled backups to the bottom of the list
        if job['disabled'] : job['nextBackupSec'] = maxAge

    for job in jobInfo:
        if job['status'] in (jobStatus.NOT_READY, jobStatus.UNKNOWN):
            # If "cycleSec" has transpired since our last backup, we're "ready".
            if job['ageSec'] > job['cycleSec'] : job['status'] = jobStatus.READY
            else : job['status'] = jobStatus.NOT_READY
        # No matter if "ready" or not, if disabled, don't back up.
        if job['disabled']: job['status'] = jobStatus.DISABLED

    sortedJobs = sorted(jobInfo, key=operator.itemgetter('nextBackupSec'))

    return sortedJobs

#-----------------------------------------------------------

def formattedTable(jobs):
    widths = { 'index':0, 'lastBackupTimestamp':0, 'ageSec':0, 'cycleSec':0 }
    for job in jobs:
        for fld in ('index','ageSec','cycleSec'):
            widths[fld] = max(widths[fld], len(str(job[fld])))
    widths['lastBackupTimestamp'] = len('2013-03-20 21:05:18')

    table = []
    table.append(''
        + 'INDEX'.center(widths['index'])                     + '   '
        + 'LAST BACKUP'.center(widths['lastBackupTimestamp']) + '   '
        + 'AGE'.center(widths['ageSec'])                      + '/'
        + 'CYCLE'.center(widths['cycleSec'])                  + '   '
        + 'STATUS')

    now = datetime.datetime.now()
    for job in jobs:
        nextBackupInterval = sec2string( job['nextBackupSec'] )
        lastBackupDurationStr = sec2string( job['lastBackupDurationSec'] )
        switch = {
            jobStatus.UNKNOWN :    '???',
            jobStatus.DISABLED :   'DISABLED',
            jobStatus.NOT_READY :  'NEXT RUN IN '+nextBackupInterval,
            jobStatus.NOT_FOUND :  'NOT FOUND',
            jobStatus.BACKING_UP : 'BACKING UP',
            jobStatus.ROTATING :   'ROTATING',
            jobStatus.READY :      'READY',
            jobStatus.SUCCEEDED :  'SUCCEEDED ('+lastBackupDurationStr+')',
            jobStatus.FAILED :     'FAILED ('+lastBackupDurationStr+')',
        }
        lastBackupTimestampString = job['lastBackupTimestamp'].strftime('%Y-%m-%d %H:%M:%S')
        table.append(''
            + job['index'].ljust(widths['index'])            + '   '
            + lastBackupTimestampString                      + '   '
            + str(job['ageSec']).rjust(widths['ageSec'])     + '/'
            + str(job['cycleSec']).ljust(widths['cycleSec']) + '   '
            + switch[job['status']]
        )
    return table

#-----------------------------------------------------------

def reportStatusAndQueue(health,activity,target,waitTime,jobInfo):

    log_debug('writing status file and queue file')
    suffix = str(os.getpid())
    cfg_datadir = config_val('dataDir', [['global']])

    # CREATE A SMALL STATUS FILE

    cfg_libdir = config_val('libDir', [['global']])
    statusFileName = cfg_libdir+'/status'
    try:
        statusFile = open(statusFileName+suffix,'w')
        now = datetime.datetime.now()
        # show time, date and PID - so we know that the info is current
        statusFile.write('date='+datetime.datetime.strftime(now,'%Y-%m-%d')+'\n')
        statusFile.write('time='+datetime.datetime.strftime(now,'%H:%M:%S')+'\n')
        statusFile.write('pid='+str(os.getpid())+'\n')
        # show what we're doing (a verb)
        statusFile.write('status='+health+'\n')
        # show what we're doing it to (an object)
        if target is None : target=''
        statusFile.write('target='+target+'\n')
        # show how long we'll do it (only if we're sleeping)
        statusFile.write('wait='+str(waitTime)+'\n')
        # show disk usage info
        statusFile.write('disk.mntpt='+cfg_datadir+'\n')
        try:
            (total, used, free) = disk_usage(cfg_datadir)
            statusFile.write('disk.mounted=true\n')
            statusFile.write('disk.total.bytes='+str(total)+'\n')
            statusFile.write('disk.used.bytes='+str(used)+'\n')
            statusFile.write('disk.free.bytes='+str(free)+'\n')
            statusFile.write('disk.used.percent='+("%.2f" % (100.0*float(used)/float(total)))+'\n')
            statusFile.write('disk.free.percent='+("%.2f" % (100.0*float(free)/float(total)))+'\n')
        except OSError:
            statusFile.write('disk.mounted=false\n')
        statusFile.close()

        # CREATE A FORMATTED TABLE

        if (jobInfo == None):
            table = ['NOT YET STARTED']
        else:
            table = formattedTable(jobInfo)
        date = datetime.datetime.strftime(datetime.datetime.now(),'%Y-%m-%d %H:%M:%S')

        # DUMP THE SORTED LIST INTO THE QUEUE FILE

        queueFileName = cfg_libdir+'/queue'
        queueFile = open(queueFileName+suffix,'w')
        queueFile.write(date+' : '+activity+'\n')
        queueFile.write('\n')
        for line in table:
            queueFile.write(line+'\n')
        queueFile.close()

        # ATOMIC WRITE

        os.rename(statusFileName+suffix, statusFileName)
        os.rename(queueFileName+suffix, queueFileName)

    except FileNotFoundError:
        return

    # SHOW THE SORTED LIST IN THE LOG

    log_info(date+' : '+activity)
    for line in table:
        log_info(line)
    log_info('')

#-----------------------------------------------------------

def do_rsync_backup(job, index, jobs):

    # if this is our first time, create a host/volume directory
    cfg_datadir = config_val('dataDir', [['global']])
    host_vol_dir = cfg_datadir+'/'+job['host']+'/'+job['volume']
    mkdir_p(host_vol_dir)

    args = [
        '-al',
        '-E',
        '-e', 'ssh -o PasswordAuthentication=no',
        '--delete',
        '--delete-excluded',
        '--numeric-ids',
        '--one-file-system',
        '--link-dest='+host_vol_dir+'/'+job['label']+'.1',
    ]
    src = job['src']
    dest = host_vol_dir+'/'+job['label']+'.0'

    ex_filename = '/tmp/excludes'
    if job['excludes']:
        with open(ex_filename, "w") as ex_fp:
            ex_fp.write("\n".join(job['excludes']) + "\n")
        args.append('--exclude-from='+ex_filename)

    if len(job['rsyncOpts']) > 0:
        log_debug('"rsyncOpts" found, adding user arguments')
        args.extend(job['rsyncOpts'].split(' '))

    cmd = ['/usr/bin/rsync'] + args + [src, dest]

    stdout_filename = host_vol_dir+'/'+job['label']+'.stdout'
    stderr_filename = host_vol_dir+'/'+job['label']+'.stderr'
    stdout_obj = open(stdout_filename, 'w')
    stderr_obj = open(stderr_filename, 'w')

    def poll_func(*args, **kwargs):
        # log_debug('args = %s, kwargs = %s' % (args, kwargs))
        reportStatusAndQueue(serverIs.BACKING_UP, "backing up '"+index+"'", index, 0, jobs)

    rc = shell_capture_2(cmd, 60, poll_func, [], {}, stdout=stdout_obj, stderr=stderr_obj)
    stdout_obj.close()
    stderr_obj.close()

    #   0      Success
    #   1      Syntax or usage error
    #   2      Protocol incompatibility
    #   3      Errors selecting input/output files, dirs
    #   4      Requested action not supported
    #   5      Error starting client-server protocol
    #   6      Daemon unable to append to log-file
    #   10     Error in socket I/O
    #   11     Error in file I/O
    #   12     Error in rsync protocol data stream
    #   13     Errors with program diagnostics
    #   14     Error in IPC code
    #   20     Received SIGUSR1 or SIGINT
    #   21     Some error returned by waitpid()
    #   22     Error allocating core memory buffers
    #   23     Partial transfer due to error
    #   24     Partial transfer due to vanished source files
    #   25     The --max-delete limit stopped deletions
    #   30     Timeout in data send/receive
    #   35     Timeout waiting for daemon connection
    complete = True if rc in (0, 24) else False
    log_debug('rc = %d'%rc + ', %s'%('OK' if complete else 'BAD') )

    try:
        os.unlink(ex_filename)
    except FileNotFoundError:
        pass

    if complete:
        prefix = cfg_datadir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
        if os.path.isdir(prefix+'0'):
            # "touch" the timestamp
            os.utime(prefix+'0',None)
        else:
            log_debug(prefix+'0 directory was not found, marking incomplete')
            complete = False
    else:
        log_error('JOB DID NOT COMPLETE')

    log_debug('backup of '+job['host']+'/'+job['volume']+' is %s' %
        ('complete' if complete else 'incomplete') )

    return complete

#-----------------------------------------------------------

def do_cp_backup(job):

    cfg_datadir = config_val('dataDir', [['global']])

    # if this is our first time, create a host/volume directory
    mkdir_p(cfg_datadir+'/'+job['host']+'/'+job['volume'])

    args = ['-alf']
    src = job['src']
    dest = cfg_datadir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.0'

    cmd = ['/bin/cp'] + args + [src, dest]
    (rc,stdout,stderr) = shell_capture(cmd)
    #   0      Success
    #   1      Failure
    complete = True if rc ==0 else False
    log_debug('rc = %d'%rc + ', %s'%('OK' if complete else 'BAD') )

    if complete:
        prefix = cfg_datadir+'/'+job['host']+'/'+job['volume']+'/'+job['label']+'.'
        if os.path.isdir(prefix+'0'):
            # "touch" the timestamp
            os.utime(prefix+'0',None)
        else:
            log_debug(prefix+'0 directory was not found, marking incomplete')
            complete = False

    log_debug('backup of '+job['host']+'/'+job['volume']+' is %s' %
        ('complete' if complete else 'incomplete') )

    return complete

#-----------------------------------------------------------

def rotate(job):

    cfg_datadir = config_val('dataDir', [['global']])
    prefix = cfg_datadir+'/'+job['host']+'/'+job['volume']+'/'+job['label']

    # rotate the numbered backups
    if os.path.isdir(prefix+'.'+str(job['keepCount'])):
        log_debug('removing '+str(job['keepCount']))
        remove_files_recursive(prefix+'.'+str(job['keepCount']))
    rotates=[]
    for i in range(job['keepCount'],0,-1):
        if os.path.isdir(prefix+'.'+str(i-1)):
            os.rename(prefix+'.'+str(i-1),prefix+'.'+str(i))
            rotates.append(str(i-1)+'>>'+str(i))
    log_debug('rotating '+('  '.join(rotates)))

#-----------------------------------------------------------

def discard(job):

    cfg_datadir = config_val('dataDir', [['global']])
    prefix = cfg_datadir+'/'+job['host']+'/'+job['volume']+'/'+job['label']
    if os.path.isdir(prefix+'.0'):
        log_debug('removing 0')
        remove_files_recursive(prefix+'.0')

#-----------------------------------------------------------

def do_single_pass():

    #write_status('thinking',0,None)
    jobs = buildJobTable()
    jobs = updateAgesAndSort(jobs)
    reportStatusAndQueue(serverIs.PREPARING, 'thinking...', None, 0, jobs)

    # GO THROUGH THE LIST IN ORDER, BACKING UP EACH ONE IF NEEDED

    log_info('start of single pass')
    for job in jobs:
        if job['ageSec'] < job['cycleSec'] : continue
        if job['disabled'] : continue

        # This is the name of our job, we'll use it a lot below.
        index = job['index']

        # First try to ping the host (full domain name) before trying to back it up.
        if ( ping(job['fqdn']) == False ):
            job['status'] = jobStatus.NOT_FOUND
            reportStatusAndQueue(serverIs.PREPARING, "'"+index+"' was not found", index, 0, jobs)
            continue

        # If we asked to probe the source directory first, look to see if the directory is there.
        if job['removable']:
            # check to see if the source is a directory (for removable media)
            (userAtHost,path) = job['src'].split(':')
            path = path.rstrip('/')
            cmd = ['/usr/bin/ssh', userAtHost, "bash -c 'if [[ -d "+path+" ]] ; then echo DIR ; fi'" ]
            (rc,stdout,stderr) = shell_capture(cmd)
            if stdout.strip() != 'DIR':
                log_info('job '+index+': probe failed, removable media not mounted')
                job['status'] = jobStatus.NOT_FOUND
                reportStatusAndQueue(serverIs.PREPARING, "'"+index+"' was not found", index, 0, jobs)
                continue
            log_info('job '+index+': probe succeeded, removable media is mounted')

        # Try to back up this job.
        job['status'] = jobStatus.BACKING_UP
        reportStatusAndQueue(serverIs.BACKING_UP, "backing up '"+index+"'", index, 0, jobs)
        startTime = datetime.datetime.now()
        if job['tool'] == 'rsync':
            if do_rsync_backup(job, index, jobs):
                job['status'] = jobStatus.ROTATING
                reportStatusAndQueue(serverIs.ROTATING, "rotating '"+index+"'", index, 0, jobs)
                rotate(job)
                job['status'] = jobStatus.SUCCEEDED
            else : # not complete
                job['status'] = jobStatus.FAILED
                reportStatusAndQueue(serverIs.CLEANING, "discarding '"+index+"'", index, 0, jobs)
                discard(job)
        elif job['tool'] == 'cp':
            if do_cp_backup(job):
                job['status'] = jobStatus.ROTATING
                reportStatusAndQueue(serverIs.ROTATING, "rotating '"+index+"'", index, 0, jobs)
                rotate(job)
                job['status'] = jobStatus.SUCCEEDED
            else : # not complete
                job['status'] = jobStatus.FAILED
                reportStatusAndQueue(serverIs.CLEANING, "discarding '"+index+"'", index, 0, jobs)
                discard(job)
        endTime = datetime.datetime.now()
        job['lastBackupDurationSec'] = (endTime - startTime).seconds
        log_debug('done')

    log_info('end of single pass')

    # sleep for a little bit
    for i in range(sleepMin,0,-1):
        jobs = updateAgesAndSort(jobs)
        reportStatusAndQueue(serverIs.IDLE, 'sleeping '+str(i)+' min', None, i, jobs)
        time.sleep(60)

#-----------------------------------------------------------

# START
def main():

    # FIRST -- PARSE COMMAND LINE
    usage = "usage: %prog [options]"
    parser = argparse.ArgumentParser(prog='flashback', description='backup manager')
    parser.add_argument("-c", "--config", action="store", dest="config_file", default="/etc/flashback.conf")
    parser.add_argument("-d", "--debug", action="store_true", dest="debug")
    parser.add_argument("-q", "--quiet", action="store_true", dest="quiet")
    parser.add_argument("-n", "--no-run", action="store_true", dest="no_run")
    global options
    # argparse.parse_args returns a Namespace object
    options = parser.parse_args()

    # SET UP SERVICES

    log_init(logfile='/tmp/flashback-init.log')

    # READ CONFIGS EARLY - THEY'RE USED EVERYWHERE

    read_config_file(options.config_file)

    # LOOK FOR PID FILE, EXIT IF FOUND

    cfg_piddir = config_val('pidDir', [['global']])
    pidfile = f'{cfg_piddir}/{PROGRAM_NAME}.pid'
    try:
        with open(pidfile) as f:
            log_info("pidfile '%s' found... better look for a running process" % pidfile)
            pid = int(f.readline())
            # Check For the existence of a unix pid, send signal 0 to it.
            try:
                os.kill(pid, 0)
            except OSError:
                log_info("pid %d not found, continuing" % pid)
            else:
                log_info("pid %d is still running, exiting" % pid)
                sys.exit()
    except FileNotFoundError:
        log_debug("no pid file found, assuming no other runners")

    with open(pidfile,'w') as f:
        f.write(str(os.getpid())+'\n')

    # RUN HELPER SCRIPT, IF PRESENT

    # The user can specify a helper script to run before flashback gets
    # started.  This would be a good place to make sure we have a proper
    # IP address, set NTP time, mount external hard disks, etc.  If the
    # helper returns 0, then we're ready to start backing up stuff.
    reportStatusAndQueue('PRESTART', 'not yet started', None, 0, None)
    cfg_helper = config_val('helper', [['global']])
    if cfg_helper:
        if not os.path.isfile(cfg_helper):
            log_info(f"helper '{cfg_helper}' not found")
            sys.exit(1)
        cmd = [cfg_helper]
        ## TODO: this hangs >> (rc,stdout,stderr) = shell_capture(cmd)
        rc = shell_do(cmd)
        if rc != 0:
            log_info(f"helper '{cfg_helper}' failed with rc {str(rc)}")
            sys.exit(1)

    # At this point, the large disk has been mounted.

    cfg_logfile = config_val('logFile', [['global']])
    log_info(f'flashback initialized, further logs will be in {cfg_logfile or "stderr"}')
    log_init(logfile=cfg_logfile)
    log_info('START')
    log_debug(f"globalCfg : {json.dumps(globalCfg, default=str)}")

    # SANITY CHECK

    cfg_datadir = config_val('dataDir', [['global']])
    if os.path.isdir(cfg_datadir) == False:
        log_debug(f"top level backup directory {cfg_datadir} not found")
        sys.exit(1)

    # SET UP SUPPORT/STATUS DIRECTORY

    cfg_libdir = config_val('libDir', [['global']])
    mkdir_p(cfg_libdir)

    # LOOP FOREVER, WORK AND SLEEP

    while True:
         do_single_pass()

    # CLEAN UP

    log_info('cleaning up')
    try:
        os.unlink(pidfile)
    except FileNotFoundError:
        pass

#-----------------------------------------------------------

def signal_handler(signal, frame):
    log_debug('received TERM signal')
    reportStatusAndQueue('DOWN', 'stopped', None, 0, None)
    sys.exit(0)

#-----------------------------------------------------------

if __name__ == "__main__":
    signal.signal(signal.SIGTERM, signal_handler)
    main()


